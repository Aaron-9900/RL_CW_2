# -*- coding: utf-8 -*-
"""CW2_RL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uKNLxgvXMILhLXGVqyhheyFC9o-t8ZIl
"""

# This is the coursework 2 for the Reinforcement Leaning course 2021 taught at Imperial College London (https://www.imperial.ac.uk/computing/current-students/courses/70028/)
# The code is based on the OpenAI Gym original (https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) and modified by Filippo Valdettaro and Prof. Aldo Faisal for the purposes of the course.
# There may be differences to the reference implementation in OpenAI gym and other solutions floating on the internet, but this is the defeinitive implementation for the course.


# Instaling in Google Colab the libraries used for the coursework
# You do NOT need to understand it to work on this coursework

# WARNING: if you don't use this Notebook in Google Colab, this block might print some warnings (do not mind them)

from IPython.display import clear_output
clear_output()

# Importing the libraries

import gym
from gym.wrappers.monitoring.video_recorder import VideoRecorder    #records videos of episodes
import numpy as np
import matplotlib.pyplot as plt # Graphical library

import torch
import torch.optim as optim
import torch.nn as nn
import torch.nn.functional as F
device = torch.device("cuda" if torch.cuda.is_available() else "cpu") # Configuring Pytorch

from collections import namedtuple, deque
from itertools import count
import math
import random
from copy import deepcopy


# Test cell: check ai gym  environment + recording working as intended

env = gym.make("CartPole-v1")
file_path = 'video.mp4'
recorder = VideoRecorder(env, file_path)

Transition = namedtuple('Transition',
                        ('state', 'action', 'next_state', 'reward'))


class ReplayBuffer(object):

    def __init__(self, capacity, k_frames=1):
        self.memory = deque([],maxlen=capacity)
        self.k_frames = k_frames
    def push(self, *args):
        """Save a transition"""
        self.memory.append(Transition(*args))
    def __len__(self):
        return len(self.memory)

    def sample(self, batch_size):
        actions, states, rewards, next_states = [], [], [], []
        for i in range(batch_size):
            random_index = np.random.randint(len(self.memory))
            
            state, action, next_state, reward = zip(self.memory[random_index])

            states.append(state[0][0])
            actions.append(action[0][0])
            rewards.append(reward[0][0])
            if next_state[0] != None:
                next_states.append(next_state[0][0])
            else: 
                next_states.append(None)


        return states, actions, rewards, next_states

    def __len__(self):
        return len(self.memory)

class DQN(nn.Module):

    def __init__(self, inputs, outputs, num_hidden, hidden_size):
        super(DQN, self).__init__()
        self.input_layer = nn.Linear(inputs, hidden_size)
        self.hidden_layers = nn.ModuleList([nn.Linear(hidden_size, hidden_size) for _ in range(num_hidden-1)])
        self.output_layer = nn.Linear(hidden_size, outputs)
    
    def forward(self, x):
        x.to(device)

        x = F.leaky_relu(self.input_layer(x))
        for layer in self.hidden_layers:
            x = F.leaky_relu(layer(x))
        
        return self.output_layer(x)
# Not used but discussed
class DQN_CNN(nn.Module):

    def __init__(self, inputs, k, outputs, batch_size,num_hidden, hidden_size):
        super(DQN_CNN, self).__init__()
        self.batch_size = batch_size
        self.input_layer = nn.Conv1d(in_channels=k, out_channels=32, kernel_size=3)
        self.post_conv_layer = nn.Linear(32*2, hidden_size)
        self.hidden_layers = nn.ModuleList([nn.Linear(hidden_size, hidden_size) for _ in range(num_hidden-1)])
        self.output_layer = nn.Linear(hidden_size, outputs)
    
    def forward(self, x):
        x.to(device)

        x = F.leaky_relu(self.input_layer(x))
        x = F.leaky_relu(self.post_conv_layer(x.view(x.shape[0], -1)))
        for layer in self.hidden_layers:
            x = F.leaky_relu(layer(x))
        
        return self.output_layer(x)

def optimize_model(memory):
    if len(memory) < BATCH_SIZE:
        return
    state_batch, action_batch, reward_batch, next_state_batch = memory.sample(BATCH_SIZE)

    # Compute a mask of non-final states and concatenate the batch elements
    # (a final state would've been the one after which simulation ended)
    non_final_mask = torch.tensor(tuple([i is not None for i in next_state_batch]), device=device, dtype=torch.bool)
    
    # Flatten to (batchsize, 4*k)
    state_batch = torch.stack([torch.flatten(a) for a in state_batch])
    action_batch = torch.stack([torch.flatten(a) for a in action_batch])
    reward_batch = torch.stack([torch.flatten(a) for a in reward_batch])


    # Can safely omit the condition below to check that not all states in the
    # sampled batch are terminal whenever the batch size is reasonable and
    # there is virtually no chance that all states in the sampled batch are 
    # terminal

    non_final_next_states = torch.stack(([i.flatten() for i in next_state_batch if i is not None]))
    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the
    # columns of actions taken. These are the actions which would've been taken
    # for each batch state according to policy_net
    state_action_values = policy_net(state_batch).gather(1, action_batch)

    # Compute V(s_{t+1}) for all next states.
    # This is merged based on the mask, such that we'll have either the expected
    # state value or 0 in case the state was final.
    next_state_values = torch.zeros(BATCH_SIZE, device=device)
    with torch.no_grad():
        # Copy net to use target network
        next_state_values[non_final_mask] = copy_net(non_final_next_states).max(1)[0]

    # Compute the expected Q values
    expected_state_action_values = (next_state_values * GAMMA) + reward_batch.flatten()
    # Compute loss
    loss = ((state_action_values - expected_state_action_values.unsqueeze(1))**2).sum()

    # Optimize the model
    optimizer.zero_grad()
    loss.backward()

    # Limit magnitude of gradient for update step
    for param in policy_net.parameters():
        param.grad.data.clamp_(-1, 1)

    optimizer.step()
    return loss

NUM_EPISODES = 150
BATCH_SIZE = 20
GAMMA = 0.995

epsilon = .3
num_hidden_layers = 3
size_hidden_layers = 100

MEM_SIZE = 10000

k = 4
J = 10
eps_decay = 0.995

# Get number of states and actions from gym action space
env = gym.make("CartPole-v1")
env = gym.wrappers.FrameStack(env, k)

env.reset()
state_dim = len(env.state)    #x, x_dot, theta, theta_dot
n_actions = env.action_space.n
env.close()

policy_net = DQN(state_dim*k, n_actions, num_hidden_layers, size_hidden_layers).to(device)
copy_net = deepcopy(policy_net)
                
optimizer = optim.Adam(policy_net.parameters())
memory = ReplayBuffer(MEM_SIZE, k)

losses = []
rewards = []

def select_action(k_steps, k, current_eps=0):
    sample = random.random()
    if sample > current_eps:
        t = torch.cat(list(k_steps)).flatten()
        with torch.no_grad():
            # t.max(1) will return largest column value of each row.
            # second column on max result is index of where max element was
            # found, so we pick action with the larger expected reward.

            return policy_net(t).argmax().view(1,1)
    else:
        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)

best_reward = 0
losses = []
rewards = []
for i_episode in range(NUM_EPISODES):
    if i_episode % 20 == 0:
        print("episode ", i_episode, "/", NUM_EPISODES, " best reward: ", best_reward)
    if i_episode % J == 0:
        copy_net.load_state_dict(policy_net.state_dict())

    # Initialize the environment and state
    
    state = torch.tensor(env.reset()).float().unsqueeze(0).to(device)
    episode_loss = []
    R = 0
    epsilon = max(epsilon * eps_decay, 0.1)
    for t in count():
        # Select and perform an action
        action = select_action(state, k, epsilon)
        next_state, reward, done, _ = env.step(action.item())
        reward = torch.tensor([reward], device=device)

        # Observe new state
        if not done:
            next_state = torch.tensor(next_state).float().unsqueeze(0).to(device)
        else:
            next_state = None

        # Store the transition in memory    
        memory.push(state, action, next_state, reward)

        # Move to the next state
        state = next_state

        R += reward
        # Perform one step of the optimization (on the policy network)
        loss = None

        if len(memory) >= k:
            loss = optimize_model(memory)
        if loss:
          losses.append(loss)
        if done:
            break
    if best_reward < R and R > 100:
        best_reward = R
        torch.save(policy_net.state_dict(), 'best_model.pth')
    rewards.append(R)
print('Complete')

env.close()

plt.plot([i for i in range(len(rewards))], rewards)

# Used for plotting stds and means. Same as above but with DDQN
def train(rb):
    global epsilon
    global env
    global policy_net
    global copy_net
    global optimizer
    global memory
    global k
    global eps_decay
    global MEM_SIZE
    global BATCH_SIZE

    eps_decay = 0.99
    k = 4
    epsilon = 0.2
    best_reward = 0
    eps_values = []
    BATCH_SIZE = 20
    MEM_SIZE = 10_000
    # Get number of states and actions from gym action space
    env = gym.make("CartPole-v1")
    env = gym.wrappers.FrameStack(env, k)

    env.reset()
    state_dim = len(env.state)    #x, x_dot, theta, theta_dot
    n_actions = env.action_space.n
    env.close()

    policy_net = DQN(state_dim*k, n_actions, num_hidden_layers, size_hidden_layers).to(device)
    copy_net = deepcopy(policy_net)
                    
    optimizer = optim.Adam(policy_net.parameters())
    memory = ReplayBuffer(MEM_SIZE, k)

    losses = []
    rewards = []
    def optimize_model(memory):
        if len(memory) < BATCH_SIZE:
            return
        state_batch, action_batch, reward_batch, next_state_batch = memory.sample(BATCH_SIZE)

        # Compute a mask of non-final states and concatenate the batch elements
        # (a final state would've been the one after which simulation ended)
        non_final_mask = torch.tensor(tuple([i is not None for i in next_state_batch]), device=device, dtype=torch.bool)
        

        state_batch = torch.stack([torch.flatten(a) for a in state_batch])
        action_batch = torch.stack([torch.flatten(a) for a in action_batch])
        reward_batch = torch.stack([torch.flatten(a) for a in reward_batch])


        # Can safely omit the condition below to check that not all states in the
        # sampled batch are terminal whenever the batch size is reasonable and
        # there is virtually no chance that all states in the sampled batch are 
        # terminal
        non_final_next_states = torch.stack(([i.flatten() for i in next_state_batch if i is not None]))
        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the
        # columns of actions taken. These are the actions which would've been taken
        # for each batch state according to policy_net
        state_action_values = policy_net(state_batch).gather(1, action_batch)

        # Compute V(s_{t+1}) for all next states.
        # This is merged based on the mask, such that we'll have either the expected
        # state value or 0 in case the state was final.
        next_state_values = torch.zeros(BATCH_SIZE, device=device)
        with torch.no_grad():
            # DDQN -> Choose action with policy_net and evaluate with copy_net
            actions = policy_net(non_final_next_states).max(1)[1]
            next_state_values[non_final_mask] = copy_net(non_final_next_states)[:, actions][:, 0]

        # Compute the expected Q values
        expected_state_action_values = (next_state_values * GAMMA) + reward_batch.flatten()
        # Compute loss
        loss = ((state_action_values - expected_state_action_values.unsqueeze(1))**2).sum()

        # Optimize the model
        optimizer.zero_grad()
        loss.backward()

        # Limit magnitude of gradient for update step
        for param in policy_net.parameters():
            param.grad.data.clamp_(-1, 1)

        optimizer.step()
        return loss
    for i_episode in range(NUM_EPISODES):
        if i_episode % 20 == 0:
            print("episode ", i_episode, "/", NUM_EPISODES)
        if i_episode % J == 0:
            copy_net.load_state_dict(policy_net.state_dict())

        # Initialize the environment and state
        
        state = torch.tensor(env.reset()).float().unsqueeze(0).to(device)
        episode_loss = []
        R = 0
        eps_values.append(epsilon)
        epsilon *= eps_decay
        for t in count():
            # Select and perform an action
            action = select_action(state, k, epsilon)
            next_state, reward, done, _ = env.step(action.item())
            reward = torch.tensor([reward], device=device)

            # Observe new state
            if not done:
                next_state = torch.tensor(next_state).float().unsqueeze(0).to(device)
            else:
                next_state = None

            # Store the transition in memory    
            memory.push(state, action, next_state, reward)

            # Move to the next state
            state = next_state

            R += reward
            # Perform one step of the optimization (on the policy network)
            loss = None

            if len(memory) >= k:
                loss = optimize_model(memory)
            if loss:
                losses.append(loss)
            if done:
                break
        if best_reward < R and R > 100:
            best_reward = R
            # torch.save(policy_net.state_dict(), 'best_model.pth')
        rewards.append(R)
    print('Complete')
    env.close()
    return rewards, eps_values

all_rewards = []
all_eps = []
for e in range(5):
    rewards, eps = train('')
    all_rewards.append(rewards)
    all_eps.append(eps)

all_rewards_1 = torch.stack([torch.cat(r) for r in all_rewards])

mean = all_rewards_1.mean(0).cpu()
std = all_rewards_1.std(0).cpu()

plt.plot([i for i in range(len(mean))], mean)
plt.fill_between([i for i in range(len(mean))], 
      mean - std, 
      mean + std, color=(0.1, 0.2, 0.5, 0.3))
plt.xlabel("nÂº of epochs")
plt.ylabel("Total reward per episode")
plt.show()

'''
k_values = [1, 2, 3, 4, 8]
for idx, mean in enumerate(means_storage):
    plt.plot([i for i in range(len(mean))], mean, label='k=' + str(k_values[idx]))
    std = std_storage[idx]

plt.legend(loc="upper left")
plt.show()
'''

## run an episode with trained agent and record video
## remember to change file_path name if you do not wish to overwrite an existing video

env = gym.make("CartPole-v1")
file_path = 'video/video.mp4'
recorder = VideoRecorder(env, file_path)

observation = env.reset()
done = False

state = torch.tensor(env.state).float().unsqueeze(0).to(device)
duration = 0
policy_net.load_state_dict(torch.load('best_model.pth'))

while not done:
    recorder.capture_frame()

    # Select and perform an action
    action = select_action([state], 1)
    observation, reward, done, _ = env.step(action.item())
    duration += 1
    reward = torch.tensor([reward], device=device)

    # Observe new state
    state = torch.tensor(env.state).float().unsqueeze(0).to(device)

recorder.close()
env.close()
print("Episode duration: ", duration)



len(rewards)

